{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Img2Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a second attempt at implementing a model that takes as input an image of a face and returns the current gesture or emotion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import torch, torchvision\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data to usable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was stored initially on a usb as multiple csv files. This script will read the data and transform it into label-folder style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/JON SNOW' #USB path, where csv files live\n",
    "train_path = 'train'\n",
    "expected_size = 4096\n",
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "# First check if train folder exists, if so, terminate.\n",
    "if os.path.exists(train_path) == False:\n",
    "\n",
    "    # Get all the files in which the data is stored\n",
    "    # This files have the format x_data0.csv, x_data1.csv, y_data0.csv, y_data1.csv, etc\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, i)):\n",
    "            if 'x_data' in i:\n",
    "                x_list.append(i)\n",
    "            if 'y_data' in i:\n",
    "                y_list.append(i)\n",
    "\n",
    "    x_list = sorted(x_list)\n",
    "    y_list = sorted(y_list)\n",
    "\n",
    "    print(x_list)\n",
    "    print(y_list)\n",
    "\n",
    "    for f_i in range(len(x_list)):\n",
    "        print(f\"Reading file n.{f_i}: {x_list[f_i]}\")\n",
    "        # Iterate over all files\n",
    "        x_path = os.path.join(path, x_list[f_i])\n",
    "        y_path = os.path.join(path, y_list[f_i])\n",
    "\n",
    "        x = open(x_path)\n",
    "        y = open(y_path)\n",
    "\n",
    "        success = None\n",
    "        while success is None:\n",
    "            try:\n",
    "                data = x.readline()\n",
    "                label = y.readline()\n",
    "\n",
    "                success = True\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        while data:\n",
    "            data = data.strip() # Remove newline\n",
    "\n",
    "            label = label.strip() # Remove newline\n",
    "            array = np.fromstring(data, sep=\",\")\n",
    "\n",
    "            # Check if data is corrupted\n",
    "            if array.shape[0] == expected_size:\n",
    "                array = array.reshape((64,64))\n",
    "                im = Image.fromarray(array).convert(\"L\") # Convert to grayscale img\n",
    "\n",
    "                label = os.path.join(train_path, label)\n",
    "                # Create label directory if doesn't exists\n",
    "                if not os.path.exists(label):\n",
    "                    os.makedirs(label)\n",
    "\n",
    "                # Get number of elements in label directory to assign name to next image\n",
    "                _, _, files = next(os.walk(label))\n",
    "                im_name = str(len(files))+'.jpeg'\n",
    "\n",
    "                # Store image\n",
    "                im.save(os.path.join(label, im_name))\n",
    "\n",
    "            # read next line for data and label\n",
    "            success = None\n",
    "            while success is None:\n",
    "                try:\n",
    "                    data = x.readline()\n",
    "                    label = y.readline()\n",
    "\n",
    "                    success = True\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        x.close()\n",
    "        y.close()\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is correctly organized, move a percentage of it into testing  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"test\"\n",
    "percentage = 0.1\n",
    "\n",
    "if os.path.exists(test_path) == False:\n",
    "    for i in os.listdir(train_path):\n",
    "        if not os.path.exists(os.path.join(test_path, i)):\n",
    "            os.makedirs(os.path.join(test_path, i))\n",
    "\n",
    "        files =  sorted(os.listdir(os.path.join(train_path, i)))\n",
    "        size = int(len(files)*percentage)\n",
    "        rand_files = np.random.choice(files, size, replace=False)\n",
    "        for file in rand_files:\n",
    "            shutil.move(os.path.join(train_path, i, file), os.path.join(test_path, i, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored in the folders `./train` and `./test`, with folder containing images, and the folder names are their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomPerspective(),\n",
    "                                transforms.RandomRotation(90),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))\n",
    "                                ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5,))\n",
    "                                    ])\n",
    "\n",
    "trainset = datasets.ImageFolder(root='./train', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.ImageFolder(root='./test', transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "data, labels = next(iter(trainloader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this are images, the model will be based on convolutions, and a final densely connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, p=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input shape is (3,64,64)\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Output shape is (6,32,32)\n",
    "        \n",
    "        # Input shape is (6,32,32)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Output shape is (12, 16, 16)\n",
    "        \n",
    "        self.fc1 = nn.Linear(12 * 16 * 16, 64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 6)\n",
    "        \n",
    "        # Add Dropout\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute activation of first conv layer\n",
    "        x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # reshape the data for fc layers\n",
    "        x = x.view(-1, 12 * 16 * 16)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        # Get the log softmax output\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 30 epochs and store the model. If a checkpoint exists, then don't train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceClassifier(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 | Loss: 2.759 | Accuracy: 20.312%\n",
      "Epoch: 0/10 | Loss: 2.752 | Accuracy: 17.188%\n",
      "Epoch: 0/10 | Loss: 2.749 | Accuracy: 32.812%\n",
      "Epoch: 0/10 | Loss: 2.743 | Accuracy: 28.125%\n",
      "Epoch: 0/10 | Loss: 2.736 | Accuracy: 29.688%\n",
      "Epoch: 0/10 | Loss: 2.734 | Accuracy: 35.938%\n",
      "Epoch: 0/10 | Loss: 2.727 | Accuracy: 32.812%\n",
      "Epoch: 0/10 | Loss: 2.721 | Accuracy: 46.875%\n",
      "Epoch: 0/10 | Loss: 2.711 | Accuracy: 32.812%\n",
      "Epoch: 0/10 | Loss: 2.706 | Accuracy: 34.375%\n",
      "Epoch: 0/10 | Loss: 2.704 | Accuracy: 39.062%\n",
      "Epoch: 0/10 | Loss: 2.698 | Accuracy: 37.5%\n",
      "Epoch: 0/10 | Loss: 2.692 | Accuracy: 31.25%\n",
      "Epoch: 0/10 | Loss: 2.686 | Accuracy: 40.625%\n",
      "Epoch: 0/10 | Loss: 2.682 | Accuracy: 42.188%\n",
      "Epoch: 1/10 | Loss: 1.019 | Accuracy: 48.438%\n",
      "Epoch: 1/10 | Loss: 1.109 | Accuracy: 31.25%\n",
      "Epoch: 1/10 | Loss: 1.192 | Accuracy: 43.75%\n",
      "Epoch: 1/10 | Loss: 1.265 | Accuracy: 31.25%\n",
      "Epoch: 1/10 | Loss: 1.329 | Accuracy: 31.25%\n",
      "Epoch: 1/10 | Loss: 1.389 | Accuracy: 43.75%\n",
      "Epoch: 1/10 | Loss: 1.444 | Accuracy: 48.438%\n",
      "Epoch: 1/10 | Loss: 1.493 | Accuracy: 43.75%\n",
      "Epoch: 1/10 | Loss: 1.534 | Accuracy: 37.5%\n",
      "Epoch: 1/10 | Loss: 1.576 | Accuracy: 40.625%\n",
      "Epoch: 1/10 | Loss: 1.616 | Accuracy: 54.688%\n",
      "Epoch: 1/10 | Loss: 1.65 | Accuracy: 45.312%\n",
      "Epoch: 1/10 | Loss: 1.682 | Accuracy: 39.062%\n",
      "Epoch: 1/10 | Loss: 1.711 | Accuracy: 45.312%\n",
      "Epoch: 1/10 | Loss: 1.739 | Accuracy: 51.562%\n",
      "Epoch: 1/10 | Loss: 1.766 | Accuracy: 54.688%\n",
      "Epoch: 2/10 | Loss: 1.019 | Accuracy: 42.188%\n",
      "Epoch: 2/10 | Loss: 1.066 | Accuracy: 48.438%\n",
      "Epoch: 2/10 | Loss: 1.107 | Accuracy: 50.0%\n",
      "Epoch: 2/10 | Loss: 1.147 | Accuracy: 56.25%\n",
      "Epoch: 2/10 | Loss: 1.184 | Accuracy: 51.562%\n",
      "Epoch: 2/10 | Loss: 1.221 | Accuracy: 46.875%\n",
      "Epoch: 2/10 | Loss: 1.255 | Accuracy: 43.75%\n",
      "Epoch: 2/10 | Loss: 1.287 | Accuracy: 42.188%\n",
      "Epoch: 2/10 | Loss: 1.319 | Accuracy: 42.188%\n",
      "Epoch: 2/10 | Loss: 1.346 | Accuracy: 42.188%\n",
      "Epoch: 2/10 | Loss: 1.373 | Accuracy: 53.125%\n",
      "Epoch: 2/10 | Loss: 1.4 | Accuracy: 57.812%\n",
      "Epoch: 2/10 | Loss: 1.424 | Accuracy: 56.25%\n",
      "Epoch: 2/10 | Loss: 1.447 | Accuracy: 50.0%\n",
      "Epoch: 2/10 | Loss: 1.469 | Accuracy: 48.438%\n",
      "Epoch: 2/10 | Loss: 1.491 | Accuracy: 54.688%\n",
      "Epoch: 3/10 | Loss: 1.023 | Accuracy: 34.375%\n",
      "Epoch: 3/10 | Loss: 1.057 | Accuracy: 50.0%\n",
      "Epoch: 3/10 | Loss: 1.089 | Accuracy: 39.062%\n",
      "Epoch: 3/10 | Loss: 1.118 | Accuracy: 53.125%\n",
      "Epoch: 3/10 | Loss: 1.146 | Accuracy: 46.875%\n",
      "Epoch: 3/10 | Loss: 1.172 | Accuracy: 45.312%\n",
      "Epoch: 3/10 | Loss: 1.196 | Accuracy: 35.938%\n",
      "Epoch: 3/10 | Loss: 1.219 | Accuracy: 46.875%\n",
      "Epoch: 3/10 | Loss: 1.243 | Accuracy: 46.875%\n",
      "Epoch: 3/10 | Loss: 1.264 | Accuracy: 51.562%\n",
      "Epoch: 3/10 | Loss: 1.285 | Accuracy: 60.938%\n",
      "Epoch: 3/10 | Loss: 1.304 | Accuracy: 50.0%\n",
      "Epoch: 3/10 | Loss: 1.322 | Accuracy: 56.25%\n",
      "Epoch: 3/10 | Loss: 1.341 | Accuracy: 43.75%\n",
      "Epoch: 3/10 | Loss: 1.359 | Accuracy: 46.875%\n",
      "Epoch: 3/10 | Loss: 1.376 | Accuracy: 35.938%\n",
      "Epoch: 4/10 | Loss: 1.018 | Accuracy: 48.438%\n",
      "Epoch: 4/10 | Loss: 1.04 | Accuracy: 53.125%\n",
      "Epoch: 4/10 | Loss: 1.061 | Accuracy: 34.375%\n",
      "Epoch: 4/10 | Loss: 1.082 | Accuracy: 50.0%\n",
      "Epoch: 4/10 | Loss: 1.102 | Accuracy: 62.5%\n",
      "Epoch: 4/10 | Loss: 1.12 | Accuracy: 59.375%\n",
      "Epoch: 4/10 | Loss: 1.138 | Accuracy: 43.75%\n",
      "Epoch: 4/10 | Loss: 1.157 | Accuracy: 59.375%\n",
      "Epoch: 4/10 | Loss: 1.175 | Accuracy: 54.688%\n",
      "Epoch: 4/10 | Loss: 1.191 | Accuracy: 54.688%\n",
      "Epoch: 4/10 | Loss: 1.207 | Accuracy: 48.438%\n",
      "Epoch: 4/10 | Loss: 1.223 | Accuracy: 56.25%\n",
      "Epoch: 4/10 | Loss: 1.238 | Accuracy: 46.875%\n",
      "Epoch: 4/10 | Loss: 1.253 | Accuracy: 43.75%\n",
      "Epoch: 4/10 | Loss: 1.267 | Accuracy: 53.125%\n",
      "Epoch: 4/10 | Loss: 1.281 | Accuracy: 54.688%\n",
      "Epoch: 5/10 | Loss: 1.017 | Accuracy: 53.125%\n",
      "Epoch: 5/10 | Loss: 1.035 | Accuracy: 65.625%\n",
      "Epoch: 5/10 | Loss: 1.051 | Accuracy: 56.25%\n",
      "Epoch: 5/10 | Loss: 1.067 | Accuracy: 48.438%\n",
      "Epoch: 5/10 | Loss: 1.082 | Accuracy: 46.875%\n",
      "Epoch: 5/10 | Loss: 1.097 | Accuracy: 54.688%\n",
      "Epoch: 5/10 | Loss: 1.111 | Accuracy: 70.312%\n",
      "Epoch: 5/10 | Loss: 1.126 | Accuracy: 64.062%\n",
      "Epoch: 5/10 | Loss: 1.139 | Accuracy: 56.25%\n",
      "Epoch: 5/10 | Loss: 1.153 | Accuracy: 48.438%\n",
      "Epoch: 5/10 | Loss: 1.166 | Accuracy: 56.25%\n",
      "Epoch: 5/10 | Loss: 1.179 | Accuracy: 51.562%\n",
      "Epoch: 5/10 | Loss: 1.191 | Accuracy: 60.938%\n",
      "Epoch: 5/10 | Loss: 1.203 | Accuracy: 60.938%\n",
      "Epoch: 5/10 | Loss: 1.216 | Accuracy: 54.688%\n",
      "Epoch: 6/10 | Loss: 1.003 | Accuracy: 62.5%\n",
      "Epoch: 6/10 | Loss: 1.017 | Accuracy: 59.375%\n",
      "Epoch: 6/10 | Loss: 1.031 | Accuracy: 51.562%\n",
      "Epoch: 6/10 | Loss: 1.044 | Accuracy: 67.188%\n",
      "Epoch: 6/10 | Loss: 1.056 | Accuracy: 56.25%\n",
      "Epoch: 6/10 | Loss: 1.069 | Accuracy: 60.938%\n",
      "Epoch: 6/10 | Loss: 1.081 | Accuracy: 57.812%\n",
      "Epoch: 6/10 | Loss: 1.094 | Accuracy: 59.375%\n",
      "Epoch: 6/10 | Loss: 1.106 | Accuracy: 70.312%\n",
      "Epoch: 6/10 | Loss: 1.117 | Accuracy: 59.375%\n",
      "Epoch: 6/10 | Loss: 1.128 | Accuracy: 60.938%\n",
      "Epoch: 6/10 | Loss: 1.138 | Accuracy: 67.188%\n",
      "Epoch: 6/10 | Loss: 1.149 | Accuracy: 68.75%\n",
      "Epoch: 6/10 | Loss: 1.16 | Accuracy: 64.062%\n",
      "Epoch: 6/10 | Loss: 1.17 | Accuracy: 54.688%\n",
      "Epoch: 6/10 | Loss: 1.18 | Accuracy: 68.75%\n",
      "Epoch: 7/10 | Loss: 1.005 | Accuracy: 51.562%\n",
      "Epoch: 7/10 | Loss: 1.017 | Accuracy: 51.562%\n",
      "Epoch: 7/10 | Loss: 1.028 | Accuracy: 50.0%\n",
      "Epoch: 7/10 | Loss: 1.039 | Accuracy: 62.5%\n",
      "Epoch: 7/10 | Loss: 1.05 | Accuracy: 68.75%\n",
      "Epoch: 7/10 | Loss: 1.06 | Accuracy: 68.75%\n",
      "Epoch: 7/10 | Loss: 1.07 | Accuracy: 59.375%\n",
      "Epoch: 7/10 | Loss: 1.08 | Accuracy: 68.75%\n",
      "Epoch: 7/10 | Loss: 1.09 | Accuracy: 43.75%\n",
      "Epoch: 7/10 | Loss: 1.1 | Accuracy: 62.5%\n",
      "Epoch: 7/10 | Loss: 1.109 | Accuracy: 51.562%\n",
      "Epoch: 7/10 | Loss: 1.118 | Accuracy: 56.25%\n",
      "Epoch: 7/10 | Loss: 1.127 | Accuracy: 57.812%\n",
      "Epoch: 7/10 | Loss: 1.136 | Accuracy: 62.5%\n",
      "Epoch: 7/10 | Loss: 1.145 | Accuracy: 68.75%\n",
      "Epoch: 7/10 | Loss: 1.154 | Accuracy: 57.812%\n",
      "Epoch: 8/10 | Loss: 1.006 | Accuracy: 54.688%\n",
      "Epoch: 8/10 | Loss: 1.017 | Accuracy: 67.188%\n",
      "Epoch: 8/10 | Loss: 1.027 | Accuracy: 68.75%\n",
      "Epoch: 8/10 | Loss: 1.036 | Accuracy: 64.062%\n",
      "Epoch: 8/10 | Loss: 1.046 | Accuracy: 54.688%\n",
      "Epoch: 8/10 | Loss: 1.055 | Accuracy: 56.25%\n",
      "Epoch: 8/10 | Loss: 1.064 | Accuracy: 62.5%\n",
      "Epoch: 8/10 | Loss: 1.073 | Accuracy: 64.062%\n",
      "Epoch: 8/10 | Loss: 1.082 | Accuracy: 62.5%\n",
      "Epoch: 8/10 | Loss: 1.09 | Accuracy: 75.0%\n",
      "Epoch: 8/10 | Loss: 1.099 | Accuracy: 57.812%\n",
      "Epoch: 8/10 | Loss: 1.107 | Accuracy: 64.062%\n",
      "Epoch: 8/10 | Loss: 1.115 | Accuracy: 64.062%\n",
      "Epoch: 8/10 | Loss: 1.123 | Accuracy: 64.062%\n",
      "Epoch: 8/10 | Loss: 1.131 | Accuracy: 57.812%\n",
      "Epoch: 8/10 | Loss: 1.139 | Accuracy: 67.188%\n",
      "Epoch: 9/10 | Loss: 1.007 | Accuracy: 67.188%\n",
      "Epoch: 9/10 | Loss: 1.015 | Accuracy: 64.062%\n",
      "Epoch: 9/10 | Loss: 1.024 | Accuracy: 67.188%\n",
      "Epoch: 9/10 | Loss: 1.032 | Accuracy: 65.625%\n",
      "Epoch: 9/10 | Loss: 1.04 | Accuracy: 64.062%\n",
      "Epoch: 9/10 | Loss: 1.047 | Accuracy: 64.062%\n",
      "Epoch: 9/10 | Loss: 1.055 | Accuracy: 76.562%\n",
      "Epoch: 9/10 | Loss: 1.063 | Accuracy: 64.062%\n",
      "Epoch: 9/10 | Loss: 1.07 | Accuracy: 50.0%\n",
      "Epoch: 9/10 | Loss: 1.077 | Accuracy: 65.625%\n",
      "Epoch: 9/10 | Loss: 1.085 | Accuracy: 71.875%\n",
      "Epoch: 9/10 | Loss: 1.092 | Accuracy: 60.938%\n",
      "Epoch: 9/10 | Loss: 1.1 | Accuracy: 54.688%\n",
      "Epoch: 9/10 | Loss: 1.107 | Accuracy: 56.25%\n",
      "Epoch: 9/10 | Loss: 1.115 | Accuracy: 64.062%\n",
      "Epoch: 9/10 | Loss: 1.122 | Accuracy: 64.062%\n"
     ]
    }
   ],
   "source": [
    "retrain = True # Whether we want to retrain a model even if a checkpoint exists\n",
    "\n",
    "if os.path.exists('checkpoint.pth') == False or retrain:\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    epochs = 10\n",
    "    steps = 0\n",
    "    train_losses, test_losses = [],[]\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "            \n",
    "            if steps % 10 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    images, labels = next(iter(testloader))\n",
    "                    ps = torch.exp(model(images))\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "                print(f'Epoch: {e}/{epochs} | Loss: {np.round(running_loss/steps+1, 3)} | Accuracy: {np.round(accuracy.item()*100, 3)}%')\n",
    "                torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceClassifier(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3072, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth copy')\n",
    "model = FaceClassifier()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model with some of the test images and check it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO1da6xdR3X+lh95Q2Ln4dz4mjhg4+BAcIJxA0FRHjxSWjV/ECpFVVpF8h9agUrVJK1UtVUrwZ8SkCokq9DmByWkjzRWgLSpmwhVQgHn4bzMtR3Hjq9fN4QYDIQktqc/zt7b33w+s+62fe8+Jnt9kuXZd+8zs2ZmzznfmvUYSykhEAi8+TFn1AIEAoFuEIs9EOgJYrEHAj1BLPZAoCeIxR4I9ASx2AOBnuCkFruZ3WxmE2a2zczumCmhAoHAzMNO1M5uZnMBbAHwEQCTAH4I4FMppedmTrxAIDBTmHcSn10DYFtKaTsAmNk9AG4BUFzsp59+ejrrrLMGDc/Lm547d25T1i+gI0eONGUzKwp06NChpnz48OHic1y/tjVnzlGyozLyNcsxf/78Yv1cn1eHgu9pHXyt8vO1V7/3HF97Y8/XnhzeePO1zlnbHyKWUevge159/Dn+DODPBdfpyVGSSevgd0nH+7XXXivWefbZZwMADh48iFdffXXoxJ/MYl8MYBddTwL4De8DZ511Fm644QYAwAUXXJDdW7BgQVPWTv3iF79oyqeffnpT1gH98Y9/3JR/+ctfFuV4/fXXh5YB4Mwzz2zKF154YXZv4cKFQ+W46KKLsud48uovtxrnnXdeUz7ttNOKn+N79UQOa5u/4ID2X4zcFn/RatsHDx4c2q5+TufijTfeGCqjjjfPNbeldXpfGFzHT3/60+zez3/+86Ftax383Kuvvprd4y9oHQMeb5af31kgnwtuC8j7uWjRoqHtAsDzzz/flPVLZ82aNQCAe++9FyWczGJvBTNbC2AtkC+kQCDQLU5mse8GsISux6u/ZUgprQOwDgDOP//8dM455wA49hePv6n0W+uMM84Yeu9Xv/pV9pz3C8LfwPyro79q/M2t3+L8i+cxDKZiWj9/w+svNo8J16l94V8v/WUvqRA6pp4aov2u4dFbvVeitNoXnkP9RT1w4EBT/slPftKU9ZeRWZze4zp5rJQ98juhfeHxqN/fGvwDxnOtP2zc3rnnnpvdY5lZDh2Pt771rU1Zx/FnP/sZAF99OJnd+B8CWG5ml5nZaQB+F8D6k6gvEAjMIk74lz2ldMjM/gjAfwGYC+DrKaVnZ0yyQCAwozgpnT2l9B0A35khWQKBwCxi1jfoGPPnz292rlVfZV1F9Q7eleR7qnexDunt+rJOproV612e2Yz3EVTH41173n1XuaamprJ7pT0HtSy88sorTVl3lVlG7pv2k/cH3vKWt2T3uN88Hro/4Jmr2vaFd7C5X0A+Pvv372/Kqpd7JteSuU3Hg68906/uE7Fe7Vk/WC5+P4B8vLk+fYd5v0DnYu/evQDydaQId9lAoCeIxR4I9ASd0vi5c+c2zjNqkvJMHwym40plmMJqHSUnD3Vc8Cghy8w0WM2I3HZNr2rs27evKTM1BXJHDI8ic19U1WAzEdNzNtsAuXqh9JPHhM2NOh6e6a2kbikFZ/PaSy+9lN1jis/1qTNSyfwF5GZFLmsd+jkG02k1h7FawvOn6kptGgOOHW9Wabl+VhWBfAz0Xj1W3tqJX/ZAoCeIxR4I9ASx2AOBnqBTnd3MGh1TzQps7lB3TdZVSsERCs9M5LnV8j3V41hPYj1d5XjmmWea8pYtW7J7paAeINeVS9FUgB8Z5UXclaDusvw5du3U8eDn2prePJ1X54L7xmPvzbvuwfA8sW6s74dncuXPqdmMZeG51aCeiYmJpqzBOiwjy6Xm6ZdffrkpayBZPYfe3kP8sgcCPUEs9kCgJ+iUxs+ZM6ehJuo9xtDIIqZHTO2UwvJz6l3H9KhUBnIaqDSK6S57UjFtB3LzmtI5psUa/cSUkCmtF22mKNE4NZt5Y8DtcdlL5nGiSTQYOh48Bp7JtZQHAMhNjFzWcfK85Lht/Rxfe+oV5zzYsWNHdo9pPcvheYFq/fWaCRofCARisQcCfUGnNP7MM8/EFVdcAeDYtD18rVSEaZXn6M90yKPxXIfSPr5WLyXeOd6zZ09TZtoO5J5rl112WfGeUl+un3etvZRPbfOlebvlSotLHno6Vjo+DP4cz4VaILgOj56z/J5VQOtn7zqWX70eeYyVInvejCUrj76nvHuuKiy/t0z3VQVkmXX91CpVeNAFAoFY7IFAXxCLPRDoCTrX2a+88koAeVpcINdRVd9h3YX1d0/nVT2Xr71EGV6SRo7Q2rp1a1Nevnx59hzr5Z43oIL1S5ZR9bNSBJ9eezok16m6OPd7bGysKaunnaezs+7oRZtxHRoFyHPtmZ1KyU2AvN8ceaZmLZ4nnXeuU8eA++P1k685bbrKxenLVbc///zzmzKnTQeOzvVsJZwMBAK/RojFHgj0BJ3S+Hnz5jU05cMf/nB27+GHH27KbNYCcgrEZpcXXnghe67kgQbkFJ9pmtJb/pyaPljVWLLkaMp89fhjzzuVg724lAaXToHRfnKSB1YthslSAlNhDcxgusi0UFUSvlZTJ1Na9oxTlYSpu44H03iWVxNDcNtve9vbsnvLli0bKpOa75gya8IRL89cKYBG6+d50px/TN05YGbx4sXZc5z4Q+e9rsPzVoxf9kCgJ4jFHgj0BLHYA4GeoFOdneGdj6Yuf6z/eOY1z420dAqoV4ea9kqutBqt5UWDcT9VR+XrUqJEAFi6dGmxfh6ftqYmlYPHoO2x0hoRx3KxCdDLye6ZGFl/V3OjZzYruR17Z855kW26x1MyCep4cNt6z9vTYHiJRj032eYz0z1gZl83sykze4b+ttDMHjKzrdX/C7w6AoHA6NGGxv8zgJvlb3cA2JBSWg5gQ3UdCAROYUxL41NK3zOzpfLnWwBcX5XvBvAIgNunq+vw4cPNET9qOmAaonnKmPqx+UHpnJfwoRRB5eWqU6rEdJq93dRUyHnKlCJz39RjjGngxRdf3JTV5MXee3yUscrMaod6dDE0WcPu3UdP3l6xYkVT9o5W8o7K8nLhefST55PNjfoZ9vJTGXfu3Dm0PjVn8jyp+ZLvecdnswqlMnLeflUTNAKvhpcQRD9Tv5uzYXpblFKqjZH7ACw6wXoCgUBHOOnd+DT4uimm+zSztWa20cw2qj9vIBDoDie6G7/fzMZSSnvNbAxAMaFcSmkdgHUAsHLlylTTKqW+TIHUc413PL0dZqXkDKZVTId0Z9RL1lCqz0v5q7SSd141IIITHDB15D4DwMqVK5uyenvxuDL1VarOqoGeNMseXrwDrDvRXn46Hke+p895VgGm5zy3SsEff/zxpqxqDb9XpX5p/UrBuS9qGWHKz/PHXohA/q4qjWeVsBSwBeQUXVWN+l2dDRq/HsCtVflWAPefYD2BQKAjtDG9fRPA9wGsMLNJM7sNwBcAfMTMtgL4cHUdCAROYbTZjf9U4dZNMyxLIBCYRXTqQffGG280Zh3Wa4Hcc0j1S9a72PSmkVae2aytRxfrkKxLqRy8r6A6GN9TPdTz3uN+l8pAfqSU6pf8LO85eAkftJ8leXWsPF2Wwf3Uo6NZz1UZWX6el/Hx8aIceuzzrl27mjLPu+6XeAktWQ4vsYWXK5/r1L0lfpbHUaP7PE/BYTIowjc+EOgJYrEHAj1BpzT+0KFDjWlEveSUkjOYzrA5wvuM533keW0xvVVqWgra8E4fVRMMe1xt2rQpu1d7F6q8Sjn5+CBNhMCfY5OgBuuwXGzKA3IPQG88PA+9Ev3XvjA4iQOQmxUnJyebsnpfshqiasL73ve+pszjoe8AmzfV9FvKpweUg6N0PDx6zqopmxvVrMrQgJm67chBFwgEYrEHAn1BLPZAoCfoVGfnqDc1P5QSQuo9rY/BepHuCbDe5bkUeqYmroPbZvMRkLtzqqsr64Pq2sl7EKzbq+7GeilHxwG56yubqNQllsdK9X6Wmc8eU33Vi2wrmUE1Wovv6Zl5nCzy6quvbsqaIPOJJ55oytu2bcvulc718/qi+yw8BjqOpZz1uifAz+n7zDEj/M5pVCRDdXaNjByG+GUPBHqCWOyBQE/QOY2vKaKXK0zh5QdjME1T8wmjbTIF14xBdXCCBCA3jam8TOHUZMdmKTbBaC50NqMprWR65+XC43tK45nicj899cc7EtrzbGT6qXnS2RuOqbvKe+211w4tq1xs2tRIRS8S0vNELEUF6niXPBuBY9+DGp6qoXLU/fHe7fhlDwR6gljsgUBP0CmNP3LkSEMzvfxuXg46poQawO/lAGOKxRROdzV5J1ZlZPrM9bN3F5DTbq2D69cdVJaRqarWwc9p4Ad7cfGu/aWXXpo9x/UrjSwd+eRRRFVXSoFH2pZ3Ii23zfIqzeZ+sjcakKs5/JzSbB43nRcvZyH3xwte8lQZVt/YEvL2t789e87znPQStzTtTvtEIBB4UyAWeyDQE8RiDwR6gs6Pf6r1FdVpWNdSs1lJH/ESN6hOxjqlZ2bha43QYpPJokVHs2dzlBiQ62TqLcX1eznIvaOK2GtOPdLY24uf0yQaXrKQNvrfdGDTEI+Hp/d7yRq4PjWbcV9074D3BDg6zkuUoXW0TXbCn1PzGstRMrUB+b6CmlVZZl0jdZ3e3MUveyDQE8RiDwR6gk5pvJk1NE5zYpcSVAA5PWIqo8krmNZ7ub+5fvVS4raURjF9ZNOHBqOwjGoKYjmUzvGzTLu1L+xppvKzXCyvmjN1jBmsGnD9qvIwvOO2uM9KkT0PPa6Tx0ZVF353WL0CcnWo5Bk4TK4SVEauk6m70n2ea8+0zKqGvn/cbzW51qpY5KALBAKx2AOBviAWeyDQE3TuLlvrV2ru8c4NKx2Z67kkeiYI79wtL288P8vPqZmF7+m+gprAGCwzuwKrPsztqd5fci0+nnPxShFxKjuPh6ez83OeqVNNapzcg/Vc3QdhvVnNoDwGPBc6Z15EmWvOcvYjGOxqrS7avOfA46Yych3qLutFjTayTveAmS0xs4fN7Dkze9bMPlv9faGZPWRmW6v/y2lDA4HAyNGGxh8C8PmU0koA1wD4jJmtBHAHgA0ppeUANlTXgUDgFEWbs972AthblQ+a2WYAiwHcAuD66rG7ATwC4HavLs4br6YfpkNKF0sJH7zc80orS2Y5pWxMA9XEU/KC8kwkXiSXgussqS5ap1JH7k/bsVJzTUmdUNlLbSmYInvyKm1lNY2f2759e/YcU1pNOMLy8zumKoOXA5/hmQ65fDzJQpjWs4qibTGNVxWzXlfemQjHtUFnZksBXAXgUQCLqi8CANgHYFHhY4FA4BRA68VuZucA+HcAn0spZSlT0+BrbKg138zWmtlGM9uoG2qBQKA7tFrsZjYfg4X+jZTSf1R/3m9mY9X9MQBTwz6bUlqXUlqdUlrt7UQHAoHZxbQ6uw2Upa8B2JxS+nu6tR7ArQC+UP1//3R1pZQavddLPKj3SrqhZ3ZSsC5TiqYCcndLT3djKGPx9HLPPbSk23qul162G85HrjnI+TnN+MP95uc8d1Y1MfKzLKPOWelMACA3qXGGn/rY7xrcN80pz+fHcRSj7qXwPotn6vT2HLisc+a5aPP+AeviOrcsh+5N1BluvCSpbezs1wL4fQBPm9mT1d/+HINFfq+Z3QZgJ4BPtqgrEAiMCG124/8PQCmH8E0zK04gEJgtdOpBN2fOnGPMWTWY2qhZruQ1p8+1PbqJ9w60DqZUXvQd03GlsCyv542l6kpJfi95ptJi9jrbs2dPUUY24yg1ZfMPU2Y193DfvLHi8VYTIMvb9ihtVTuYxntqAr87ai7le1o/y6/7Tiyzp3p55xGU+qZrhce7FNXp5fYP3/hAoCeIxR4I9ASdJ6+odxiV9jG91d1tpmIlWgb4XlylQBsNqmD6pfeYVnkBHEx3tZ9ennSm2kwJNd+Yl5+ckxrw5zRAhE+e1fr5FFp+TvvJqpdSU945Zhl1TFlepfg8jl7+OPZAU1XDU2UY3rzwvHvvre6yM7yddL72PPm83Im1WqKnBjPilz0Q6AlisQcCPUEs9kCgJ+hcZy/pNaxrqN7BepJ37lbJawvI9R3WDTnPOpDrf6yv6udYf1UdjL24PBOMZybhtrwc+3wMMQBs2bKlKbMZRyOtWJfV+nft2tWUWc9VkxT3TfdL2LTFn9u6dWv23Pe//32UcOWVVzZl7/hpnkN9d1jvPXDgQFNWExrLq2YtL3qQ9XneL/Dy16u+zf3hd0Lr4LXD+eWBYz0khyF+2QOBniAWeyDQE3R+/FNNedsGrQBls4iXw9urkz+ntMzLdVaK2vM8v7SfTPm9fpYSGgA5BVfaytesymiACNP/qak8YHFiYqIpj4+PN2U+ihrI1RylpqWEI9pnzrnvJTThfum887Wa3rg9HlMOEgJydUXlYG9DVR25/lL+d31O5ed3opRvH8jHWMfbO1areWbaJwKBwJsCsdgDgZ4gFnsg0BN0qrOnlBr9RHUfvla3Ri/SjeG5b/I16zeezq7mO9bDuD5P11RwP3UMuG3WsVUvZx3ey6HOuqa6XrKLrOqv3B826eieBeuNWj/rqN6RzZdccsnQ+oA8YQWbOnXcuE4vKSb3S8fNe8c8cy/vR5R0b4XuLZXy0numt5LOHlFvgUAgFnsg0Bd0bnqrodSlFNkGlPNqKWXxkleUaLw+5x55W6D/Hq1U8LNKF5lKsplIjwtiCq4edEz1mH5qW17eeM7bxtRd54Hpv9JnvuY69Dn2QGO1A8hNe/ycjgerPF7kI78v7E0H5O+cmilZRdG55XH13p1SMg+gnNDEy1FYSnwSRzYHAoFY7IFAXzCyQBil6ky/lBaXTk/1vJm8lLreibFewAK3x/Lrc6W2gLyfHrXm+r2jm5S2lrz8lN55SRLY6sD0Wevwdn5Lz2lbpTTK+iy/A14dqiYw3eX502QepbkFcmuIzjWPj3eiK8vsvVfeu+RRfM+Ls/n8tE8EAoE3BWKxBwI9QSz2QKAn6FxnV12mRtuc4az76Gc877TScyrPyy+/3JTZgwvI9SQ2k3ln2OnegRddxffYw0tNTQzVc9lsxgkOVBfkY650T6CkG2oec9aVdRxLpk6Vwxs71p15bFRfZZOdeqdxXnqeM5XXy23fdn+G77VNPgnkY+V6wDmeiJ7JrfnMdA+Y2Rlm9gMz22Rmz5rZX1d/v8zMHjWzbWb2LTNrdzBaIBAYCdrQ+NcA3JhSei+AVQBuNrNrAHwRwJdSSssAvALgttkTMxAInCzanPWWANSccn71LwG4EcDvVX+/G8BfAfjqNHU1piKl2W2TWXgeaHxP6TPTNqaESre2b9/elJXGM9VjuqU02zORsFxeUAVTTs/0pvWXZFTqyHRfvfCYtvL4KI3n3HIqRylgyTsOS9UynjOvL2xu09z2pTx5nK9eP6ey8/hrjn2WywtU8Y7KKtF4XRM8HqqGzFjyCjObW53gOgXgIQDPAziQUqpnahLA4jZ1BQKB0aDVYk8pHU4prQIwDmANgMvbNmBma81so5lt1F+oQCDQHY7L9JZSOgDgYQAfAHCemdVcZRzA7sJn1qWUVqeUVuvOcSAQ6A7T6uxmdiGAN1JKB8zsTAAfwWBz7mEAnwBwD4BbAdw/XV1Hjhxp9EEvQYDq26W83ZrkwksewDqOlwBjcnKyKS9btiy7x3odlzVKinU33RPwTCs8Bt54lNyHgVyP5nuaN54jypRxcUQYy6FtlZItqow8Btp/3h9QPbR01pvqp6z3q/swt8254T2d3XNx9s4h9Nx2TySxio6p547bJnlFGzv7GIC7zWwuBkzg3pTSA2b2HIB7zOxvATwB4Gst6goEAiNCm934pwBcNeTv2zHQ3wOBwK8BOvWgmzNnTkN1lPoyPVKTVOmel7zCu8cUSNvasWNHU9boJ4a32eh5A7LZSCknR1exB53KyLRvbGwsu1c6CknNZqzKaNIIpqDeccvcFzU/sjrB8qo588UXX2zKeqQRzxM/t3Llyuw5blvVJpaD+8ymRyA3PypVZ7mUgvN74B3tzHJ5x3hzW6queJ6ItUrrvXvhGx8I9ASx2AOBnqBTGn/kyJGGIilV8gJESrvs3jFAJwquX2VkKuZ5LHlH/Xgnn5YSeCg1844BKrWtFJypr3dKLN9TCrtv376mrEdDsTrE9JYDjYBchdDx4GOu2Jrw3HPPZc8x9VWrA7fHY6Mn0qpayeC58CwSXpILzwOwbXCY52VatxeppAOBQCz2QKAviMUeCPQEnevstanCO3ZJPeNYZ/KOtOVr1Xfa6vMlHQzIdS2uT3WuE9XZea/CS5jpmWBKyQs1TzrnfFcZ+VmuT8eDwaYx4FhzXg0vIaRG37Fe7SXz4Dq1Dv7cggULmrKOG+9NqN7P+nzb/PvLly/PnuN7ah5kWdomTfXy45cQv+yBQE8Qiz0Q6Ak6P8W1ph+e6U0pStvTMfmeUqCSqcwzbyj1ZRMPU3etuxTQMt29NnnEAD8RAtNzrp9zsQH56axKaZmuczIMVVeYZrOZDMgpM5vetC2m3dr/d77znU358ccfb8pq5mNPxy1btmT3OICGVUWl+zxW6m3INF7fK54Lfm9LaswwlHLbe1Dvzvo9iOOfAoFALPZAoC+IxR4I9ASd6+y1TuJF/qheVDqK2dOHPbNFqW6tQ/W68fHxpsz6q3fss6eXq65fSjaodXDf1P109+6jCYN4X0QTJbJ+6Z31xmPwnve8J3uOI+zWrMmjnScmJprydddd15RVZ1+xYkVTfvrpp7N7V111NLKaE4moaZZ17O9973vZPR7HJ598silPTU1lz/FYeRmVLrnkkuyao/i4LZVRE2EySvOuY1U6MlzrKCF+2QOBniAWeyDQE4zM9OZFtrU1SenfS552Xh0Kz4OO77U9Ptfz5FOaxtde9B2PldJzNr0xleScc0BOi9Wk9u53v7sps3lNPb8uvfTSpsz53YBcNWCqq6qLd3QT54m74oormvJ3v/vd7LmPfexjTdnzSty2bVtT1sg5NjF6nple3kM+Blrnlr389H3h8feST3BbKkc9T6UIOiB+2QOB3iAWeyDQE3RO42v64dEtj8pofQwvAMULkinVoQkfSqqAUrZSamDAPyKI1RCm56rysPeU0ni+5r5o3rarr766KeuO/vvf//6mzPRfvdMWLz56CJDKyJYLpuraZ06AoZ5rrL7s3bu3KbPFAQCeeuqppsxJM1QO9oBkFQQAlixZMvQ5IH9X1ULD4HyAmhzDO/21lD5ax7SUVAQ46rEYND4QCMRiDwT6gljsgUBP0Hnyilr/UZ3dS7DI8I60ZXhH2rY9HlplLB135B3xpLoam6TUc43b83Qv1svVPKgmmRrqFcYJGp5//vnsXikPu2ca0z2Sb3/72035pptuasoaHccRZV/5yleyexdffHFTZr1cdWo2+6kuu3bt2qbMXmwf/OAHs+cWLVrUlPX94zHlvQMgH1f2rtP9Hob3/vF+j5cIVN/Nem9lRnT26tjmJ8zsger6MjN71My2mdm3zOy06eoIBAKjw/HQ+M8C2EzXXwTwpZTSMgCvALhtJgULBAIzi1Y03szGAfwWgL8D8Cc24Gw3Avi96pG7AfwVgK969bAHnWc2804E9fKYe0creYktGF5ADntgcZCGtsVyeF5y7LUF5MENrHYoBWcaqzndmJJ7Odnvu+++pqzmqs2bj36nP/bYY01Zc7Px+GhfONDknnvuKcrB5jatn+fimmuuacpM74HcvKb3eKzYVKhzy/OiagKrNSU1CchVKqXT/Dmvfu6z0nhWIWYzecVdAP4MQL0izwdwIKVUK9qTABYP+2AgEDg1MO1iN7PfBjCVUnpsumcLn19rZhvNbKN3LnogEJhdtKHx1wL4HTP7OIAzALwVwJcBnGdm86pf93EAu4d9OKW0DsA6ADjnnHPaRaMEAoEZR5vz2e8EcCcAmNn1AP40pfRpM/tXAJ8AcA+AWwHc36bBWldqG4U2jWzZNev9aiYqmfO8vO7qispJG1nP8lxz1V3WM9mxaYj1V09GjgYD8gQTnDBTdTzO884mNAVHcnnMTHO5s+7MxyNrskiuX2XkfQBOYKnHLbPpTc2ZLDPvF3B9QD7Guv/Adej+SekocO+d0MQTJVdxPX/O2xOo3x3vDMKTcaq5HYPNum0Y6PBfO4m6AoHALOO4nGpSSo8AeKQqbwewxns+EAicOug86q2mLG295OrPTVfWOj06x1A65OXt5sQQTH2V9jE99/LHKeXiepjSewkwmAYDOc1kjytta/Xq1UX52RzG8mpU2jve8Y6mrCoJyzU5OdmUle6ziW7Xrl3ZPZaZ59NLWqJysGmSyxqV5r2P3J6+V6X3UU173LaqK6Xc8+qtxxF3Omf1vM8WjQ8EAr9GiMUeCPQEI6PxSsXanrLqgalYW5u+R5FVRqagTE01vTDDo5wqYymARlUNPhZJaSXXz5RQ1RMvYIKTWfCuuh6HtWnTpqas1JTl4nue2uTl8mMPNKWq3JbSc/YiZOqrO+Je0giWQ+kzw3uHvRyLPNe8487vmMql1oR6HOP4p0AgEIs9EOgLYrEHAj1B5zp7KeLMMxm0TUDJ8I7W9Y5W8pJccJ1sJrr88suz59jMolFvXlRTWzMRm9TUo4u9y9j8pV5y3LZGovE151fXI4x4rDRijb2/PLMTm8q8KEYeA40a47Y1CpDHm8uefu29E9p2qS3tC+8ReMknS+ZdwH8nal3fi+iMX/ZAoCeIxR4I9ASd0njgKEVSGuWZvPian1Pq79F9potMt1xThVM/U2n1/OK21NTE7an5p0RblT4zRVbaxhScKb72xctnVjI16Vh5edZYRvXyY3Cf1eTF91gONTfyO6F1sLriBUpxnTqmPHZqeuM6PTMi16nvBI8j59HX94MDflRd2b9//zHyKOKXPRDoCWKxBwI9QSz2QKAn6Fxnr+Edi9v2uGXPXHc8CS1L8PQfNve88MIL2T02z3gRcaq7sY5dcvNUqPmOI45XMpcAAA6xSURBVKP4nkas8dlmmkSR+82RV14iUAXr6d5xyKzbcp+B3ERVOs4a8PXh0r6FJiZhHVhl5LbV1Mn7HTxunulX90hqfRvIowAV3nvQJilM/LIHAj1BLPZAoCfonMbXdOZ4otxK1MTL86U0qpSU4njk4Pr5KGOl8Rwdxs8BuWlIKSfTblYF1NTEpjg1y3FkHtNF7SePh3q18dhxTnZ9jumt3mNKzn1u6+Gm4Lbazq3e4/lT0xtTZPVwY3qu6hCPK8+nF9mmKgRHGfI9fT+8+j2Vtnlm2icCgcCbArHYA4GeoFMab2YNjfOol9Ko0smWXuIJL4+d15ZHJe+6666mzLvZDz74YPbcI4880pSV4nPQjLbFQRBMF/mEUSC3BGhONwbTPi9RhlJTBtfv0VulnOxBx/XrnDGd9nbqvdyDnlcly+y9H2yRUFWDZdT3tqQq6Y47v8NK49niwd50Y2Nj2XPcF00k8q53vQsAMDExgRLilz0Q6AlisQcCPUEs9kCgJ+hcZ691Ks/DzY3ccTztSvUBuWmlbSJGNjsBZW8sTdzAcqluxUdIeckgWIdUUxNHP6kJia9Z31bTGF97UW+eV1gp4kvhHXPMfda9g5Ke7plcvfFoezyTwksSypFpXFbPRt7D2LNnT3aPrz3zMb/DKke9D+Alr2h7PvsOAAcBHAZwKKW02swWAvgWgKUAdgD4ZErplVIdgUBgtDgeGn9DSmlVSqk+SuQOABtSSssBbKiuA4HAKYqTofG3ALi+Kt+NwRlwt0/3oZLHmheowtSGqZgXEKHgOtiE4VFYpaYbN25synwaqXfapiYg4KQUehop943NM6oKsDpROs0TyCmseuHxtdJsHkceH4/qemPPpiyl2aV5AfK+8ee0jlLAjD7L93Q8WH7PvKb0vETddd45QIkDX4A8+IXNbUrVPRNpPYczEQiTAPy3mT1mZmurvy1KKdUGwn0AFg3/aCAQOBXQ9pf9Qyml3WZ2EYCHzOxHfDOllMxs6FdK9eWwFjj22zQQCHSHVr/sKaXd1f9TAO7D4Kjm/WY2BgDV/0MDcVNK61JKq1NKq70UuoFAYHYx7eozs7MBzEkpHazKHwXwNwDWA7gVwBeq/+9v02CtU3guj6qTlZ5T8xrrWppXm/Uu1dNL0CNz169f35RXrVrVlDXhAOtk2k+tk8E6GevpeuaXl8ecr7k+HVPW+3U8eBy945D5y9tLRsK6t37hey6mJfdW3aco6eUqR9uz2FQv52u9x+ZNz9TJiSQ5yg3Ix5jnTOvw8tKX1hWjzU/tIgD3VQM1D8C/pJQeNLMfArjXzG4DsBPAJ1vUFQgERoRpF3tKaTuA9w75+8sAbpoNoQKBwMyj8+OfapqhtK8tdWe0pX3aHtNA78ghvccUnE1cSrM9KqVH+jDYE8/zpOLndNz4nkezPWpdOiZJ22KTmm6+cnueR6SnlrXNFchtef0seSgCOT3XSMKSlxyQm0jZBKs59fkYLa2/rbmU2/K8GUsI3/hAoCeIxR4I9ASx2AOBnqDzqLdaR1PzCesnXpYZ1lVUh/TOUWNdtlQfkOtyGpXGJpIXX3yxKXumPNWHuQ412bGMXKe6TWqUHYPHhM1wmn3F07dL/fH2WTwfCu6zlyjR27fx4GWqKe3jqEusF9nG46HvFevpnGmI3WMBYMeOHU3Zy+HP76O6U3Pbmr++Hn/PvBi/7IFATxCLPRDoCTr3X63phlJHptZK9ZjaME3ROlg1ULrF97xkCgz1lmK09cLzIsW0n6XEgwrviGIeR85Zv2DBguw5zuvuRYp5SUW8o6/52kviWWoL8E2pJXgmKJZJ58+LWGOTFycfAfLEE0zV2WMO8FUNVrd27tzZlDmyEsijKVWVq9WyoPGBQCAWeyDQF3S+G19TRs9rq60nlUc/leKXqCTncwN8LyUG79RrwIKnkng0i6k7U0mVkfutO7tM71h18YJHdBecd6a5Ld3B5n56J7p6FpS2OQU9eKpGKSkKU2Ign0NNRuIlnmDavWXLlqa8e/fu7DnOZ6hjxTLyOQOrV6/OnuP346KLLsLxIn7ZA4GeIBZ7INATxGIPBHqCzqPear3PSy6oKB2nq7qyZ94oHRusZhYvWqukX6pZi80zqud6ub9Zt+W2t23blj3n5Xzn9ji6SvvJbS1cuLC1jAxvT6Dkkebtg+h+Bs+TF6nonQPAbfNzmkCC77EnHJCb1LZu3ZrdY92c93s80/LSpUuzexw1yeY2L6pTzcd1spYwvQUCgVjsgUBfMLLjnzzzmlKRUoCEZ9byKLiX7ICvlcKWTDxMe4HcLKeUcyaSOnB+urbye552SvG5P0yltS2mqqqWMQVltUPHgz+nJkaWy8vrzlAZ+VmmvuqhyJ5wbE4D8oAlTTxRUh21n54XHpv6WCXUvrCZVQOb6nuex2b8sgcCPUEs9kCgJ4jFHgj0BJ3q7HPmzGlMBBpR5kWRcaC+l+DgRM4i83RelYmvPdfO0jHBgB8pxmBd0Euwwbqgtsef0zr4Wk2YrBt60XGs23umJm8/hqHmsFKSEd3D8I5iZr2f9W1NEsrJSPQev6u698FzwfJ6OfbVbMYysunN29MprZ+ZOOstEAj8miMWeyDQE3RK4+fOndtEZSl9ZorsHUPsRcdxnZo/jutk+qn0lk013hFV3JaacUrJNoa1V4JnxvHMcjw+TFs13x1/TqkpJ1PgvimNZ+ruJRIp5ezXOlWdKHnoeYkylCKzmZJNXGpe00g3Bpu5lD6XkoyoGZHfR+/YMo5a1HnxcvnNGI03s/PM7N/M7EdmttnMPmBmC83sITPbWv2/YPqaAoHAqNCWxn8ZwIMppcsxOApqM4A7AGxIKS0HsKG6DgQCpyjanOJ6LoDrAPwBAKSUXgfwupndAuD66rG7ATwC4Pbp6qspnXc8k3fMEH9OPdf4c3qPqSk/p7SPqa9HOdlCoCezMsXSOtxABceawPDoHF9z35QSMh3VMWAvLs5jp1Sd50UtF6XTUz3PSa2jFAijfSntuAN5kgq+p/Sb5dJ+lhJgqCyeZyaPgcrIdfA75qlvmrRkpnLQXQbgJQD/ZGZPmNk/Vkc3L0op1W/5PgxOew0EAqco2iz2eQCuBvDVlNJVAH4Boexp8HU3dGfAzNaa2UYz29g2q2sgEJh5tFnskwAmU0qPVtf/hsHi329mYwBQ/T817MMppXUppdUppdXqvB8IBLpDm/PZ95nZLjNbkVKawOBM9ueqf7cC+EL1//0t6mr0sra5xIFjdSiuj8F6tOqGrMN7xxWXoqT0WTalqP7EOqTqUJ5OVvI6U3gJFhmersx6o5fog2VUcybvg3j7JwzvqGR9J3isWH410bVN5sH1156cNdgTUev39j5YRi/BBkOjDHmseM70HeC+8V7KsGeHoa2d/Y8BfMPMTgOwHcAfYsAK7jWz2wDsBPDJlnUFAoERoNViTyk9CWD1kFs3zaw4gUBgttD58U813fBOcdUTKpkiennmmJopjWL6xdRO9xGYLmqSBK6TaZMexcN0S2llifYNa68EL18fy8Vj2jZHHJBTWvZAUxrP4600vpQ3UOeFabwX5MSf0zHlOfMCm7yTZr389W1pPMuvcnjysxro0XGWQ+ezVj08tS584wOBniAWeyDQE8RiDwR6gs4TTpbc+rzzy1iv9nQS1lFV/yvpXV7udo1wYnMV1+e5V3q5vz3wc9oXvuftK7DeqHsT3hHWbHriudC22BVVo8ZK8+klhPT6ydA6eI9H54Lr5/nUpI+lqEggN5V57r5ch/aF3x017fHeB9en/eR7Op9a5zDEL3sg0BPEYg8EegI70WNyT6gxs5cwcMC5AMCPp3l8tnEqyACEHIqQI8fxynFpSunCYTc6XexNo2YbU0rDnHR6JUPIEXJ0KUfQ+ECgJ4jFHgj0BKNa7OtG1C7jVJABCDkUIUeOGZNjJDp7IBDoHkHjA4GeoNPFbmY3m9mEmW0zs86y0ZrZ181sysyeob91ngrbzJaY2cNm9pyZPWtmnx2FLGZ2hpn9wMw2VXL8dfX3y8zs0Wp+vlXlL5h1mNncKr/hA6OSw8x2mNnTZvakmW2s/jaKd2TW0rZ3ttjNbC6AfwDwmwBWAviUma3sqPl/BnCz/G0UqbAPAfh8SmklgGsAfKYag65leQ3AjSml9wJYBeBmM7sGwBcBfCmltAzAKwBum2U5anwWg/TkNUYlxw0ppVVk6hrFOzJ7adtTSp38A/ABAP9F13cCuLPD9pcCeIauJwCMVeUxABNdyUIy3A/gI6OUBcBZAB4H8BsYOG/MGzZfs9j+ePUC3wjgAQA2Ijl2ALhA/tbpvAA4F8ALqPbSZlqOLmn8YgC76Hqy+tuoMNJU2Ga2FMBVAB4dhSwVdX4Sg0ShDwF4HsCBlFIdOdLV/NwF4M8A1BEm549IjgTgv83sMTNbW/2t63mZ1bTtsUEHPxX2bMDMzgHw7wA+l1L6Gd/rSpaU0uGU0ioMflnXALh8tttUmNlvA5hKKT3WddtD8KGU0tUYqJmfMbPr+GZH83JSadunQ5eLfTeAJXQ9Xv1tVGiVCnumYWbzMVjo30gp/ccoZQGAlNIBAA9jQJfPM7M6NrWL+bkWwO+Y2Q4A92BA5b88AjmQUtpd/T8F4D4MvgC7npeTSts+Hbpc7D8EsLzaaT0NwO8CWN9h+4r1GKTABlqmwj5Z2CBA+2sANqeU/n5UspjZhWZ2XlU+E4N9g80YLPpPdCVHSunOlNJ4SmkpBu/D/6aUPt21HGZ2tpm9pS4D+CiAZ9DxvKSU9gHYZWYrqj/VadtnRo7Z3viQjYaPA9iCgX74Fx22+00AewG8gcG3520Y6IYbAGwF8D8AFnYgx4cwoGBPAXiy+vfxrmUBcCWAJyo5ngHwl9Xf3w7gBwC2AfhXAKd3OEfXA3hgFHJU7W2q/j1bv5sjekdWAdhYzc1/AlgwU3KEB10g0BPEBl0g0BPEYg8EeoJY7IFATxCLPRDoCWKxBwI9QSz2QKAniMUeCPQEsdgDgZ7g/wEogs3aBjhigAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: happy | 85% | True: happy\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = ['neutral','happy','surprise','angry','sad','disgust']\n",
    "\n",
    "\n",
    "idx = np.random.randint(len(testset))\n",
    "image, label = testset.__getitem__(idx)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "ps = torch.exp(model(image))\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "image = (image+1)/2\n",
    "plt.imshow(np.rollaxis(image.numpy().squeeze(),0,3))\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predict: {classes[top_class.item()]} | {int(top_p.item()*100)}% | True: {classes[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces from default camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use OpenCV to detect faces from our default camrea stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2, time, face_recognition\n",
    "from PIL import Image\n",
    "\n",
    "# Create an object. Zero for external camera\n",
    "video = cv2.VideoCapture(0)\n",
    "cv2.startWindowThread()\n",
    "scale = 0.25\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))\n",
    "                                ])\n",
    "\n",
    "# Start face recognition algorithm\n",
    "face_cascade = cv2.CascadeClassifier('/anaconda3/envs/jupyter/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    # Create a frame object\n",
    "    check, frame = video.read()\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    width = int(gray.shape[1]*scale)\n",
    "    height = int(gray.shape[0]*scale)\n",
    "    \n",
    "    small_gray = cv2.resize(gray, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Look for faces\n",
    "    # faces = face_cascade.detectMultiScale(gray, 2, 5)\n",
    "    faces = face_recognition.face_locations(small_gray)\n",
    "    \n",
    "    # Insert face locations in frame\n",
    "    for (top,right,bottom,left) in faces:\n",
    "        # rescale for drawing\n",
    "        top = int(top*(1/scale))\n",
    "        right = int(right*(1/scale))\n",
    "        bottom = int(bottom*(1/scale))\n",
    "        left = int(left*(1/scale))\n",
    "        cv2.rectangle(frame,(left, top), (right, bottom), (255,0,0), 2)\n",
    "        \n",
    "        # Extract faces and predict an emotion\n",
    "        face = gray[top:bottom, left:right]\n",
    "        face = cv2.resize(face, (64,64), interpolation=cv2.INTER_AREA)\n",
    "        # face = np.array([face, face, face])\n",
    "        face = Image.fromarray(face.astype(np.uint8))\n",
    "        face = transform(face)\n",
    "        face = torch.cat((face,)*3, 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ps = torch.exp(model(face.unsqueeze(0)))\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "        emotion = classes[top_class]\n",
    "        text = f\"{int(top_p.item()*100)}% | {emotion}\"\n",
    "        \n",
    "        # Draw a label with the emotion predicted\n",
    "        cv2.rectangle(frame, (left, bottom-25), (right, bottom), (255,0,0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, text, (left + 6, bottom - 6), font, 0.5, (255,255,255), 1)\n",
    "        \n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Shutdown the camera\n",
    "video.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyWindow(\"Camera\")\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
