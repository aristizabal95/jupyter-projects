{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Deep Learning PATE Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The Private Aggregation of Teaher Ensembles_ (PATE) has been designed to allow unlabeled, unsensitive data to be automatically labeled through the combined opinion of multiple private teacher models (Papernot et al.). This framework ensures privacy for the teacher's data and models, but requires the student to share their unlabeled dataset for analysis. This may be unreasonable in cases where the student's data also requires privacy, as in the case of a Hospital labeling the X-rays of multiple patients, or a company wanting to extract information from product usage by sharing their data to their competitors. This notebook proposes considering the aggregated opinion of teacher models as an encrypted service, where both the teachers and the student data is protected using multi-party computation and additive secret sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 14:16:52.483600 4745418176 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/anaconda3/envs/jupyter/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0813 14:16:52.499740 4745418176 deprecation_wrapper.py:119] From /anaconda3/envs/jupyter/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import syft as sy\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.22a1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we're going to use a simple dataset, such as MNIST. Since the objective of this demo is not to prove a high-performance model, but a privacy framework for multiple parties. The objective here is to use the testing dataset as our sensitive unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(15),\n",
    "    transforms.RandomPerspective(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data/train', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = datasets.MNIST('./data/test', train=False, download=True, transform=test_transforms)\n",
    "testloader = th.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the workers and distribute the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to eliminate trust between data holders in the PATE Framework. For this reason, we need to define both the teacher workers and a trusted party, which will be considered as a neutral worker not owned by any involved party. This worker will be responsible for the encryption of the unlabeled data, as well as the encryption of the teacher models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teachers = 10\n",
    "teachers = tuple(sy.VirtualWorker(id=str(i), hook=hook) for i in range(num_teachers))\n",
    "secure_worker = sy.VirtualWorker(id=\"secure_worker\", hook=hook)\n",
    "\n",
    "# Split the dataset into equal-sized partitions for all the teachers\n",
    "# trainsets = tuple(th.utils.data.random_split(train_dataset, [len(train_dataset)//num_teachers]*num_teachers))\n",
    "idxs = np.random.permutation(len(train_dataset))\n",
    "split_size = len(train_dataset)//num_teachers\n",
    "\n",
    "trainsplits = tuple((train_dataset.data[idxs[i:i+split_size]],train_dataset.targets[idxs[i:i+split_size]]) for i in range(num_teachers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the teacher's models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to later use this models for encrypted computation, we must make sure it's implemented as such. Because of this, we can't use `log_softmax` inside our model definition. We'll use it inside the training loops and use `argmax` instead during inferation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        \n",
    "        # Input shape is (1,28,28) => 784\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        \n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        \n",
    "        self.fc7 = nn.Linear(16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reshape the data for fc layers\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        \n",
    "        # Get the linear output. Classification is done outside the model.\n",
    "        x = self.fc7(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the teacher models on disjoint data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real life situation, teachers would train privately in their local computers. In this case, we're going to train them all in our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainsets = tuple(\n",
    "    sy.BaseDataset(\n",
    "        trainsplits[i][0].copy(), \n",
    "        trainsplits[i][1].copy(), \n",
    "        transform=train_transforms)\n",
    "    for i in range\n",
    "    (num_teachers))\n",
    "\n",
    "teacher_models = list(\n",
    "        # Models are stored in a list since we have to reassign them later\n",
    "        TeacherModel()\n",
    "    for i in range(num_teachers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(teacher_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    if not os.path.exists(f'teacher_{i}_chkpt.pth') or retrain:\n",
    "        epochs = 20\n",
    "        trainloader = th.utils.data.DataLoader(trainsets[i], shuffle=True, batch_size=64)\n",
    "        model = teacher_models[i]\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            steps = 0\n",
    "            for images, labels in trainloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Use log_softmax for local classification\n",
    "                log_ps = F.log_softmax(model(images))\n",
    "                loss = criterion(log_ps, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                steps += 1\n",
    "\n",
    "                if steps % 20 == 0:\n",
    "                    print(f'Teacher {i}/{num_teachers} | Epoch: {e}/{epochs} | Loss: {np.round(running_loss/steps+1, 3)}')\n",
    "        else:\n",
    "            th.save(model.state_dict(), f'teacher_{i}_chkpt.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple PATE Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the trained teacher models, we can label our unlabeled data (which for this demois the MNIST test dataset) byh combining the opinions of all the teachers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialize the models and move them to their designated teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're now inferring, we can send the models to our workers, and simulate a real life scenario were the teachers have their trained models inside their machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_models = list(\n",
    "        # Models are stored in a list since we have to reassign them later\n",
    "        TeacherModel()\n",
    "    for i in range(num_teachers))\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    chkpt_path = f'teacher_{i}_chkpt.pth'\n",
    "    if os.path.exists(chkpt_path):\n",
    "        state_dict = th.load(chkpt_path)\n",
    "        teacher_models[i].load_state_dict(state_dict)\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    teacher_models[i] = teacher_models[i].send(teachers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get noisy opinions from each of the teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send our unlabeled data to the teachers, so that they can generate predictions for each datapoint. Note here two things:\n",
    "1. We must send our data to the teacher. This means our data is compromised.\n",
    "2. We move the teacher's opinion to a secure worker. This ensures the student (our local machine) has no access to the raw data. Therefore, privacy is conserved\n",
    "\n",
    "To ensure that the opinions are differentially private, we add laplacian noise with a certain epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions = None\n",
    "\n",
    "unlabeled_data, labels = next(iter(testloader))\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    unlabeled_data = unlabeled_data.send(teachers[i]) # send the data to teacher\n",
    "    \n",
    "    ps = th.exp(teacher_models[i](unlabeled_data)) # get teacher's opinion\n",
    "    _, top_class = ps.topk(1, dim=1)\n",
    "    \n",
    "    top_class.move(secure_worker) # Move the teacher's opinion to a secure worker.\n",
    "    \n",
    "    if opinions is None:\n",
    "        opinions = top_class\n",
    "    else:\n",
    "        opinions = th.cat((opinions, top_class), dim=1) # concatenate all opinions\n",
    "    \n",
    "    unlabeled_data = unlabeled_data.get() # retrieve the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the opinions tensor consists of a matrix representing all the opinions the teachers gave for each datapoint. In order to ensure privacy from this conclusions, we count the votes for each datapoint and return the value with the highest number of votes. Also, we add Laplacian noise to the vote counts so that we can ensure an epsilon-delta differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_argmax(x, epsilon=0.1):\n",
    "\n",
    "    # First get the vote count for each datapoint.\n",
    "    labels = th.stack([th.bincount(x_i, minlength=10).long() for x_i in th.unbind(x, dim=0)], dim=0)\n",
    "\n",
    "    # Add Laplacian noise to the votecount.\n",
    "    beta = 1 / epsilon\n",
    "    noise = th.from_numpy(np.random.laplace(0, beta, labels.shape))\n",
    "    \n",
    "    labels = labels.double() + noise.send(labels.location)\n",
    "\n",
    "    # Then get the highest votecount index\n",
    "    labels = th.argmax(labels, dim=1)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_labels = noisy_argmax(opinions, epsilon=1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the noisy aggregated opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is already labeled, we can check how much the noise aggregation affected the accuracy of the predictions. Feel free to change the epsilon value on the `noisy_argmax()` query to see how the accuracy changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = labels == noisy_labels\n",
    "accuracy = th.mean(equals.float())\n",
    "print(f\"Noisy Argmax Accuracy: {int(accuracy*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these labels to train our student model without leaking privacy from neither the private datasets nor the teachers' models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making PATE Framework bidirectionally private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe the code above, the student is required to send their data to the teachers. This implies that the student data can't hold private information. This is also pointed out by Papernot et. al. on the paper [*_Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data_*](https://arxiv.org/pdf/1610.05755.pdf), were is stated that _\"using auxiliary, unlabeled non-sensitive data, a student model is trained on the aggregate output of the ensemble, such that the student learns to accurately mimic the ensemble\"_. This is a strong and somewhat unrealistic assumption, since if the teacher datasets require privacy, most probably the student unlabeled dataset also holds this requirement. Following the hospitals example, the student dataset would also hold personal and private information, which cannot be legally leaked to any other hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure privacy both for the teachers and the student, we can modify our noisy argmax query so that it behaves as an encrypted service. This implies using secret additive sharing to encrypt both the teacher models and the unlabeled dataset. By doing so, we're able to generate predictions on the dataset without leaking the raw student data to any of the teachers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialize the models and move them to their designated teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to the original situation. We have pre-trained models inside our teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_models = list(\n",
    "        # Models are stored in a list since we have to reassign them later\n",
    "        TeacherModel()\n",
    "    for i in range(num_teachers))\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    chkpt_path = f'teacher_{i}_chkpt.pth'\n",
    "    if os.path.exists(chkpt_path):\n",
    "        state_dict = th.load(chkpt_path)\n",
    "        teacher_models[i].load_state_dict(state_dict)\n",
    "\n",
    "for i in range(num_teachers):\n",
    "    model = teacher_models[i]\n",
    "    # model = model.fix_precision().share(alice, bob, crypto_provider=secure_worker)\n",
    "    teacher_models[i] = model.send(teachers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federate the teacher models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to federate the teachers' models into multiple workers. Even though this could be achieved using any of the workers already created, we're going to add `alice` and `bob` to maintain clarity.\n",
    "\n",
    "Also, since Syft's encryption protocol is based on SecureNN's implementation, multi-party computation must be done between 3 workers: one crypto provider and 2 holding shares of the tensor.\n",
    "\n",
    "In order to encrypt each model, we must use fixed precision and secret additive sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = sy.VirtualWorker(id='alice', hook=hook)\n",
    "bob = sy.VirtualWorker(id='bob', hook=hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of PySyft 0.1a22, there seems to be a problem when doing `fix_precision()` with models. Because of this, it has to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_teachers):\n",
    "    model = teacher_models[i]\n",
    "    # Need to do fix_precision manually because of a current bug\n",
    "    for p in model.parameters():\n",
    "        # p is a pointer tensor to the parameter value. Get remote id and location\n",
    "        p.data = p.data.fix_precision().share(alice, bob, crypto_provider=secure_worker)\n",
    "        # p.location[p.id_at_location].child.child = p.location[p.id_at_location].child.child.wrap()\n",
    "        # print(p.location[p.id_at_location].child.child.wrap())\n",
    "    # teacher_models[i] = model.fix_precision().share(alice, bob, crypto_provider=secure_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we inspect the parameters of any of these models, we'll see that they're shared among two teachers. The model is still present inside each teacher, so to see the encryption we have to do `copy().get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:31671520625 -> alice:81626926948]\n",
       " \t-> [PointerTensor | me:30151621494 -> bob:32120256602]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:45266953502 -> alice:86828236915]\n",
       " \t-> [PointerTensor | me:74567199126 -> bob:21625440265]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:55639373297 -> alice:26323743630]\n",
       " \t-> [PointerTensor | me:54223525621 -> bob:80420824177]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:67135734779 -> alice:92963732345]\n",
       " \t-> [PointerTensor | me:52084213213 -> bob:81571445801]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:92477685334 -> alice:36599167633]\n",
       " \t-> [PointerTensor | me:69310350484 -> bob:22057464204]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:98756345077 -> alice:52609674034]\n",
       " \t-> [PointerTensor | me:82719988138 -> bob:43116283331]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:88376750083 -> alice:2352139945]\n",
       " \t-> [PointerTensor | me:75160029683 -> bob:35108485729]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:84965812266 -> alice:48942275109]\n",
       " \t-> [PointerTensor | me:48804233873 -> bob:54167635901]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:10203553353 -> alice:81815713485]\n",
       " \t-> [PointerTensor | me:65675810585 -> bob:97409493460]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:13063937769 -> alice:93003422351]\n",
       " \t-> [PointerTensor | me:22201706069 -> bob:73907655453]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:33356843081 -> alice:66711878787]\n",
       " \t-> [PointerTensor | me:89823063016 -> bob:52959858981]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:71311859276 -> alice:74870866191]\n",
       " \t-> [PointerTensor | me:44139277190 -> bob:92661879467]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:48791229472 -> alice:11292497713]\n",
       " \t-> [PointerTensor | me:67840306743 -> bob:46850835499]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:10704148802 -> alice:83515349880]\n",
       " \t-> [PointerTensor | me:86988839928 -> bob:25948346179]\n",
       " \t*crypto provider: secure_worker*]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(teacher_models[0].copy().get().parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypt the unlabeled dataset and obtain the teachers opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also encrypt the student's unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = unlabeled_data.fix_precision().share(alice, bob, crypto_provider=secure_worker)\n",
    "unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain predictions from our data without needing to trust on any of the teachers. Looking closely at the code, we're still \"sending\" the data to each teacher, but remember that the data is encrypted along `alice` and `bob`. This step is only required to allow computation between the data and the models.\n",
    "\n",
    "Also, performing encrypted operation takes a long time, so instead of generating predictions through the whole dataset, we'll do it with batches, and join them together as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/100 | Getting predictions on teacher 8\r"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "testloader = th.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "opinions = None\n",
    "\n",
    "for i, (images, _) in enumerate(testloader):\n",
    "    \n",
    "    # Create a buffer with dimensions (num_teachers, batch_size)\n",
    "    # This batch will later on be transposed.\n",
    "    batch_opinions = th.zeros((num_teachers, len(images)))\n",
    "    \n",
    "    # Encrypt the buffer\n",
    "    batch_opinions = batch_opinions.fix_prec().share(alice, bob, crypto_provider=secure_worker)\n",
    "    \n",
    "    # Encrypt the images\n",
    "    images = images.fix_prec().share(alice, bob, crypto_provider=secure_worker)\n",
    "\n",
    "    # Get opinions on the batch along every teacher\n",
    "    for t in range(num_teachers):\n",
    "        print(f\"Batch {i}/{len(test_dataset)//batch_size} | Getting predictions on teacher {t}\", end=\"\\r\")\n",
    "        model = teacher_models[t].copy() # Not sure why but I must copy the model, else it wont run\n",
    "        teacher_opinions = None\n",
    "        \n",
    "        # Send the encrypted data to the teacher. Privacy is still preserved\n",
    "        images = images.send(teachers[t])\n",
    "\n",
    "        # Get teacher's opinion\n",
    "        output = model(images) \n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        # We have our predictions, let's retrieve the data\n",
    "        images = images.get()\n",
    "        \n",
    "        # Let's now store our predictions inside our buffer\n",
    "        \n",
    "        # First move the buffer to the required worker\n",
    "        batch_opinions = batch_opinions.send(teachers[t])\n",
    "        \n",
    "        # Assign the value at the worker index\n",
    "        batch_opinions[t] = pred\n",
    "        \n",
    "        # We have stored the predictions, let's retrieve the buffer.\n",
    "        # The buffer is still encrypted\n",
    "        batch_opinions = batch_opinions.get()\n",
    "        \n",
    "    # By now we have all the teachers' opinions stored in our buffer\n",
    "        \n",
    "    # We're no longer indexing by teacher, let's transpose the buffer\n",
    "    batch_opinions.transpose_(1, 0)\n",
    "    \n",
    "    # In order to apply noisy_argmax, we must unencrypt the buffer.\n",
    "    # Let's first send it to secure_worker, so we can't see the raw predictions\n",
    "    batch_opinions = batch_opinions.send(secure_worker)\n",
    "    \n",
    "    # Now let's remotely unencrypt with remote_get\n",
    "    batch_opinions = batch_opinions.remote_get()\n",
    "    \n",
    "    # We must return the buffer into float_precision. Currently doing\n",
    "    # float_prec() on pointers is not supported, so it must be done\n",
    "    # manually.\n",
    "    batch_opinions = batch_opinions.owner.send_command(\n",
    "        batch_opinions.location, \n",
    "        (\"float_prec\", batch_opinions, (), {})\n",
    "    ).wrap()\n",
    "    \n",
    "    # Now, to use noisy_argmax, our tensor must be of type LongTensor\n",
    "    batch_opinions = batch_opinions.long()\n",
    "    \n",
    "    # Finally, let's get our noisy labels with noisy_argmax\n",
    "    noisy_labels = noisy_argmax(batch_opinions, 0.8)\n",
    "    \n",
    "    # This labels are already differentially private, so we can see them now.\n",
    "    noisy_labels = noisy_labels.get()\n",
    "    \n",
    "    # Concatenate all opinions\n",
    "    if opinions is None:\n",
    "        opinions = noisy_labels\n",
    "    else:\n",
    "        opinions = th.cat((opinions, noisy_labels)) \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opinions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the noisy aggregated opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the accuracy of the original PATE Framework with the encrypted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = labels == noisy_labels\n",
    "accuracy = th.mean(equals.float())\n",
    "print(f\"Noisy Argmax Accuracy: {int(accuracy*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's no difference in terms of accuracy! By encrypting, we're compromising computational time for privacy guarantees with the student data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
