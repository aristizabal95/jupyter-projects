{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build an encrypted model based on a pretrained model. This will allow us to use already functioning models at our advantage, will retaining protection and privacy both for the data holder and the model owner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:\n",
    " - Alejandro Aristiz√°bal - Github: [@aristizabal95](https://github.com/aristizabal95)\n",
    " \n",
    "This notebook is based on Pysyft's [official tutorial on Encrypted NN](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2012%20-%20Train%20an%20Encrypted%20Neural%20Network%20on%20Encrypted%20Data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workers will share both the data and the model in an encrypted manner, using Additive Sharing Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import syft as sy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 11:24:06.805057 4536808896 hook.py:98] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Set the workers and their hooks\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "alice = sy.VirtualWorker(id=\"alice\", hook=hook)\n",
    "bob = sy.VirtualWorker(id=\"bob\", hook=hook)\n",
    "james = sy.VirtualWorker(id=\"james\", hook=hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll be using a dataset composed of photos of flowers, provided by Kaggle. Get it with [this link](https://www.kaggle.com/alxmamaev/flowers-recognition/downloads/flowers-recognition.zip/2), save it in the home directory of this notebook and extract the zip file to get the directory `flowers/`. **Make sure the directory has this exact name**\n",
    "\n",
    "**Note:** This dataset contains images of varying sizes, with an average of 320x240 pixels. We will later deal with this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(\"flowers\"), \\\n",
    "\"It appears like there's no folder called \\\"flowers\\\". \\\n",
    "Did you follow the instructions above?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data obtained from kaggle isn't split between training and testing. This script will do that for you ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'flowers'\n",
    "train_path = 'flowers/train'\n",
    "test_path = 'flowers/test'\n",
    "test_percent = 0.1 # 10% of the data will be used for testing\n",
    "\n",
    "##### CREATING THE TRAINING SPLIT ######\n",
    "\n",
    "\n",
    "# First check if a training path doesn't exist\n",
    "if not os.path.exists(train_path):\n",
    "    # Create the training path\n",
    "    print(\"Training folder doesn't exist. Creating it\")\n",
    "    os.makedirs(train_path)\n",
    "\n",
    "    \n",
    "# Now check if the train folder is empty\n",
    "if [f for f in os.listdir(train_path) if not f.startswith('.')] == []:\n",
    "    print(\"Training folder is empty. Moving data to it\")\n",
    "    datafolders = os.listdir(source_path)\n",
    "    \n",
    "    # Exclude the train and test folders if they already exists\n",
    "    datafolders = list(set(datafolders) - set(['train', 'test']))\n",
    "    \n",
    "    for folder in datafolders:\n",
    "        shutil.move(os.path.join(source_path, folder), train_path)\n",
    "        \n",
    "\n",
    "##### CREATING THE TESTING SPLIT ######\n",
    "\n",
    "\n",
    "# First check if a testing path doesn't exist\n",
    "if not os.path.exists(test_path):\n",
    "    # Create the training path\n",
    "    print(\"Testing folder doesn't exist. Creating it\")\n",
    "    os.makedirs(test_path)\n",
    "    \n",
    "\n",
    "# Now check if the test folder is empty\n",
    "if [f for f in os.listdir(test_path) if not f.startswith('.')] == []:\n",
    "    print(\"Testing folder is empty. Moving data to it\")\n",
    "    \n",
    "    # Move data from the training set to the testing set\n",
    "    for folder in os.listdir(train_path):\n",
    "        if not os.path.exists(os.path.join(test_path, folder)):\n",
    "            os.makedirs(os.path.join(test_path, folder))\n",
    "            \n",
    "        files = sorted(os.listdir(os.path.join(train_path, folder)))\n",
    "        size = int(len(files)*test_percent)\n",
    "        rand_files = np.random.choice(files, size, replace=False)\n",
    "        for file in rand_files:\n",
    "            shutil.move(os.path.join(train_path, folder, file), os.path.join(test_path, folder, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Transform our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using PyTorch's ImageFolder class to load our data.\n",
    "The pretrained models expect the input data to have some properties (e.g. WidthxHeight, normalization, etc. For that, we're going to use transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
